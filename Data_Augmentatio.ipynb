{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwwTjUWmd4LY"
   },
   "source": [
    "#Install Library"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YiiyEbUdeMFW",
    "outputId": "4113d391-eaba-4b0f-c7d3-e6e92ed65353"
   },
   "source": [
    "pip install grad-cam -q"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LsxRzu8TeEnb",
    "outputId": "a380592c-f162-4283-f4b9-c6de0f0796ce"
   },
   "source": [
    "# Install Timm (Need to restart the runtime after finish install )\n",
    "!pip install git+https://github.com/rwightman/pytorch-image-models.git\n",
    "!pip install lightning transformers datasets evaluate pillow==9.2.0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0A0s1NVmeQMM"
   },
   "source": [
    "⚠ ☝ Restart runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w979qmUverdT"
   },
   "source": [
    "##Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "M0IHRiA8e5hr",
    "outputId": "f3e3c504-9152-49f8-aa9c-6f828e9a3584"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Td7PTn8PfATK"
   },
   "source": [
    "!cp /content/drive/MyDrive/Augmentation_Datasets/test.zip .\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jPMadzrpe35D",
    "outputId": "e042650b-a258-4324-b40d-c44c9a27c66c"
   },
   "source": [
    "!unzip -q '/content/drive/MyDrive/Datasets/test.zip' -d '/content/drive/MyDrive/Datasets/test.zip'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoP5_3GuH0w2"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2xHxWft7fRjD"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "# Pytorch Image model (TIMM) library: a library for state-of-the-art image classification\n",
    "import timm\n",
    "import timm.optim\n",
    "import timm.scheduler\n",
    "from timm.data import ImageDataset, create_dataset, create_loader\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "\n",
    "from lightning.fabric import Fabric\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "import shutil\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSA4nws7fbd4",
    "outputId": "6c61b02b-9266-46f5-fedf-45562d3d4168"
   },
   "source": [
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6f3f0e38b66c46cd8372daf77db532d9",
      "91b9d63c698b46a0872a424d4e4466fe",
      "fcd4cfa89d6a4bd4b98af6f66dff973f",
      "50c1108f20a841cc89c50a9bc93659c2",
      "aa9f9897019c4b9096324ffbc7fc531d",
      "000ab3594fb24b6a9fa874a5fe27bd53",
      "f1e62898af8347dea698e427a24e231c",
      "c663a41174b84d00a2d998b490cf7e7b",
      "d6a7a98667d24dc0a529b03d14eee8af",
      "3ef803dbb220477092e0feb59e7e1a7b",
      "3a145e65d1544b6fa721fea4dc998bbf"
     ]
    },
    "id": "x4WXr7zmBnHd",
    "outputId": "7d73fa69-ebd9-4275-8ad8-a283013f1f49"
   },
   "source": [
    "# Select model (List of available is shown above)\n",
    "mobilenetv3_large_100 =  timm.create_model('efficientnet_checkpoint_fold0.pt', pretrained=True)\n",
    "\n",
    "# Modify the model for your number of classes\n",
    "mobilenetv3_large_100.classifier = nn.Linear(in_features=mobilenetv3_large_100.classifier.in_features, out_features=4)\n",
    "\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=mobilenetv3_large_100, # Pass the model instance directly\n",
    "        input_size=(16, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQjBl_N9f_-7"
   },
   "source": [
    "#Grand CAM"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EGFTriK_glW7"
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import requests\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    deprocess_image, \\\n",
    "    preprocess_image\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSuqz0nQf0QY",
    "outputId": "3942c575-b684-4f41-96be-00c93e158f67"
   },
   "source": [
    "import os\n",
    "\n",
    "source_dir = '/content/test'\n",
    "\n",
    "img_paths = []\n",
    "\n",
    "for root , dir, files in os.walk(source_dir):\n",
    "  for file in files:\n",
    "    file_path = os.path.join(root,file)\n",
    "    img_paths.append(file_path)\n",
    "print(len(img_paths))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LxZFVnK_EAWR"
   },
   "source": [
    "import os\n",
    "# Define the directory paths\n",
    "test_ad = '/content/drive/MyDrive/Datasets/3_cls/test/AD'\n",
    "test_control = '/content/drive/MyDrive/Datasets/3_cls/test/CONTROL'\n",
    "test_pd = '/content/drive/MyDrive/Datasets/3_cls/test/PD'\n",
    "\n",
    "train_pd = '/content/drive/MyDrive/Datasets/3_cls/train/PD'\n",
    "train_ad = '/content/drive/MyDrive/Datasets/3_cls/train/AD'\n",
    "train_control = '/content/drive/MyDrive/Datasets/3_cls/train/CONTROL'\n",
    "\n",
    "print(\"Test Ad: \",len(os.listdir(test_ad)))\n",
    "print(\"Train Ad: \",len(os.listdir(train_ad)))\n",
    "print(\"Test pd: \",len(os.listdir(test_pd)))\n",
    "print(\"Train pd: \",len(os.listdir(train_pd)))\n",
    "print(\"Train control: \",len(os.listdir(train_control)))\n",
    "print(\"Test control: \",len(os.listdir(test_control)))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JkJh00ysED95"
   },
   "source": [
    "model_path = '/content/efficientnet_checkpoint_fold0.pt'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOtz2wK4h_9T",
    "outputId": "d2b8f81d-bad7-428a-aedb-d095a36c987c"
   },
   "source": [
    "# ใช้ timm เพื่อสร้างโมเดลและโหลดพารามิเตอร์ pretrained\n",
    "model = timm.create_model('tf_efficientnetv2_b0.in1k', pretrained=True)\n",
    "model.eval()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "id": "7Yi4bWvElWH5",
    "outputId": "bc3e49c8-69a0-4879-a916-7dbf8c5c8d58"
   },
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# โหลดโมเดล\n",
    "model = timm.create_model('tf_efficientnetv2_b0.in1k', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# โหลดและเตรียมภาพ\n",
    "image_url = \"/content/test/AD/AD_2680.png\"\n",
    "img = np.array(Image.open(image_url))\n",
    "img = cv2.resize(img, (224, 224))\n",
    "\n",
    "# เช็คว่าภาพเป็น grayscale หรือไม่ ถ้าใช่ให้แปลงเป็น RGB\n",
    "if img.ndim == 2:  # Grayscale image\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "img = np.float32(img) / 255\n",
    "input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "print(input_tensor)\n",
    "print(input_tensor.shape)\n",
    "\n",
    "# กำหนด target สำหรับ Grad-CAM\n",
    "targets = [ClassifierOutputTarget(0)]  # เลือกคลาสที่คุณสนใจ เช่น 0\n",
    "\n",
    "# เลือก target layer ที่เหมาะสมจากโมเดล\n",
    "target_layers = [model.blocks[-1]]  # ตัวอย่างการเลือกเลเยอร์ที่เหมาะสม\n",
    "\n",
    "# สร้าง Grad-CAM\n",
    "with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "    grayscale_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "    print(grayscale_cams)\n",
    "\n",
    "    # แปลงเป็นภาพ RGB และแสดงผล\n",
    "    cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "\n",
    "# แปลง grayscale_cams เป็นภาพ RGB\n",
    "cam = np.uint8(255 * grayscale_cams[0, :])\n",
    "cam = cv2.merge([cam, cam, cam])\n",
    "\n",
    "# ผสานภาพต้นฉบับและ CAM เข้าด้วยกัน\n",
    "images = np.hstack((np.uint8(255 * img), cam, cam_image))\n",
    "\n",
    "# แสดงภาพ\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axs[0].imshow(np.uint8(255 * img))\n",
    "axs[0].set_title('Original Image')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(cam)\n",
    "axs[1].set_title('Grad-CAM')\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(cam_image)\n",
    "axs[2].set_title('Overlay Image')\n",
    "axs[2].axis('off')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q54gnHlsieWf"
   },
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import timm  # Import timm to load the model directly\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "\n",
    "# Load the model directly from timm\n",
    "model = timm.create_model('tf_efficientnetv2_b0.in1k', pretrained=True)\n",
    "model = model.eval()\n",
    "model = model.to('cpu')  # Move the model to CPU if needed\n",
    "\n",
    "# ... rest of your code (from cell 53)\n",
    "image_url = \"/content/test/AD/AD_2680.png\"\n",
    "img = np.array(Image.open(image_url))\n",
    "# ... (continue with the rest of your code)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mwJ1g9_QgXp9"
   },
   "source": [
    "def grand_cam(img_path, model, class_idx):\n",
    "    # Load image and apply transforms\n",
    "    img = np.array(Image.open(img_path))\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "\n",
    "    # Check if the image is grayscale and convert to RGB if necessary\n",
    "    if img.ndim == 2:  # Grayscale image\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    img = np.float32(img) / 255\n",
    "    input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # Grad-CAM Process\n",
    "    targets = [ClassifierOutputTarget(class_idx)]\n",
    "    target_layers = [model.blocks[-1]]  # Using the last block of EfficientNetV2\n",
    "\n",
    "    with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "        grayscale_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "        cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "\n",
    "    cam = np.uint8(255 * grayscale_cams[0, :])\n",
    "    cam = cv2.merge([cam, cam, cam])\n",
    "    images = np.hstack((np.uint8(255 * img), cam, cam_image))\n",
    "    pil_image = Image.fromarray(images)\n",
    "    print(f\"Image : {os.path.basename(img_path)}\")\n",
    "    plt.imshow(pil_image)\n",
    "    plt.pause(0.1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "j8BGLKyMh5rv",
    "outputId": "aa2bf8f0-e566-4a6d-98a6-4fc74e74f9dc"
   },
   "source": [
    "# Call the function with the model and image path\n",
    "grand_cam('/content/test/AD/AD_2680.png', model, 0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IVZab4XLLkA_"
   },
   "source": [
    "def grand_cam(img_path, model, class_idx):\n",
    "    # Load image and apply transforms\n",
    "    img = np.array(Image.open(img_path))\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "\n",
    "    # Check if the image is grayscale and convert to RGB if necessary\n",
    "    if img.ndim == 2:  # Grayscale image\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    img = np.float32(img) / 255\n",
    "    input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # Grad-CAM Process\n",
    "    targets = [ClassifierOutputTarget(class_idx)]\n",
    "    target_layers = [model.blocks[-1]]  # Using the last block of EfficientNetV2\n",
    "\n",
    "    with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "        grayscale_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "        cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "\n",
    "    cam = np.uint8(255 * grayscale_cams[0, :])\n",
    "    cam = cv2.merge([cam, cam, cam])\n",
    "    images = np.hstack((np.uint8(255 * img), cam, cam_image))\n",
    "    pil_image = Image.fromarray(images)\n",
    "    print(f\"Image : {os.path.basename(img_path)}\")\n",
    "    plt.imshow(pil_image)\n",
    "    plt.pause(0.1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQmuj0wc4Yzw"
   },
   "source": [
    "#AD"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wPDSFkRFkjOa",
    "outputId": "278193d9-bd34-4fd1-b2c5-29349001755b"
   },
   "source": [
    "# Directory path and corresponding class index\n",
    "image_directory = '/content/test/AD'\n",
    "image_paths = [os.path.join(image_directory, filename) for filename in os.listdir(image_directory) if filename.endswith('.png')]\n",
    "\n",
    "for img_path in image_paths:\n",
    "  grand_cam(img_path,model,0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdXJkjnp4eQL"
   },
   "source": [
    "#CONTROL"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZKcGpZJWnKgE",
    "outputId": "135e334d-c518-41b7-89bf-00520284565f"
   },
   "source": [
    "# Directory path and corresponding class index\n",
    "image_directory = '/content/test/CONTROL'\n",
    "image_paths = [os.path.join(image_directory, filename) for filename in os.listdir(image_directory) if filename.endswith('.png')]\n",
    "\n",
    "for img_path in image_paths:\n",
    "  grand_cam(img_path,model,1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fFbNqNK4hLq"
   },
   "source": [
    "\n",
    "#PD"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fBYlJjMzpoJK",
    "outputId": "d1a1728c-cf07-4adc-c57d-310a7bc630ff"
   },
   "source": [
    "image_directory = '/content/test/PD'\n",
    "image_paths = [os.path.join(image_directory, filename) for filename in os.listdir(image_directory) if filename.endswith('.png')]\n",
    "\n",
    "for img_path in image_paths:\n",
    "  grand_cam(img_path,model,2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S8z2O1GVtSYC"
   },
   "source": [
    "def grand_cam(img_path, model, class_idx):\n",
    "    # Load image and apply transforms\n",
    "    img = np.array(Image.open(img_path))\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "\n",
    "    # Check if the image is grayscale and convert to RGB if necessary\n",
    "    if img.ndim == 2:  # Grayscale image\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    img = np.float32(img) / 255\n",
    "    input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # Grad-CAM Process\n",
    "    targets = [ClassifierOutputTarget(class_idx)]\n",
    "    target_layers = [model.blocks[-1]]  # Using the last block of EfficientNetV2\n",
    "\n",
    "    with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "        grayscale_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "        cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "\n",
    "    cam = np.uint8(255 * grayscale_cams[0, :])\n",
    "    cam = cv2.merge([cam, cam, cam])\n",
    "    images = np.hstack((np.uint8(255 * img), cam, cam_image))\n",
    "    pil_image = Image.fromarray(images)\n",
    "    print(f\"Image : {os.path.basename(img_path)}\")\n",
    "    plt.imshow(pil_image)\n",
    "    plt.pause(0.1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "7SgxcazRO74f",
    "outputId": "e9316c78-78dd-41e1-b194-aae74462b6ef"
   },
   "source": [
    "img ='/content/test/PD/PD_308.png'\n",
    "grand_cam(img, model, 3) # Added img as the first argument"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4UFEOjrOt0ur",
    "outputId": "67f225e0-0662-42d8-9bb7-22ccb7251f52"
   },
   "source": [
    "image_directory = '/content/test/AD'\n",
    "image_paths = [os.path.join(image_directory, filename) for filename in os.listdir(image_directory) if filename.endswith('.png')]\n",
    "\n",
    "for img_path in image_paths:\n",
    "  grand_cam(img_path,model,0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lRhIk1K1uJtC",
    "outputId": "c91293fb-97d6-45ac-9f0f-bacebbb17cd8"
   },
   "source": [
    "image_directory = '/content/test/CONTROL'\n",
    "image_paths = [os.path.join(image_directory, filename) for filename in os.listdir(image_directory) if filename.endswith('.png')]\n",
    "\n",
    "for img_path in image_paths:\n",
    "  grand_cam(img_path,model,1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-8bt_FgDdJ85",
    "outputId": "d2319024-eeb4-4930-88e9-eef725b637e5"
   },
   "source": [
    "image_directory = '/content/test/PD'\n",
    "image_paths = [os.path.join(image_directory, filename) for filename in os.listdir(image_directory) if filename.endswith('.png')]\n",
    "\n",
    "for img_path in image_paths:\n",
    "  grand_cam(img_path,model,2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ugf_qIGilvFf"
   },
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# กำหนด transforms สำหรับการทำ Data Augmentation\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),  # ครอบตัดภาพให้มีขนาด 224x224 แบบสุ่ม\n",
    "    transforms.RandomHorizontalFlip(),  # กลับด้านภาพแนวนอนแบบสุ่ม\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # ปรับค่าความสว่าง สีสัน ฯลฯ\n",
    "    transforms.ToTensor(),  # แปลงภาพให้เป็น Tensor\n",
    "    # Removed the Lambda function that repeats the grayscale image across 3 channels\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ปรับค่า Normalize\n",
    "])\n",
    "\n",
    "# โหลดและเตรียมภาพพร้อมการทำ Data Augmentation\n",
    "image = Image.open(image_url).convert('RGB') # Ensure image is in RGB format\n",
    "image_augmented = data_augmentation(image)\n",
    "\n",
    "# แปลง Tensor กลับเป็นรูปภาพสำหรับการแสดงผล\n",
    "image_augmented = image_augmented.permute(1, 2, 0).numpy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "tKcfUhEhl6CQ",
    "outputId": "f0874747-2328-447c-dc6c-8bdaaeee809d"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# โหลดและเตรียมภาพพร้อมการทำ Data Augmentation\n",
    "image = Image.open(image_url).convert('RGB')  # Ensure image is in RGB format\n",
    "image_augmented = data_augmentation(image)\n",
    "\n",
    "# แปลง Tensor กลับเป็นรูปภาพสำหรับการแสดงผล\n",
    "image_augmented = image_augmented.permute(1, 2, 0).numpy()\n",
    "\n",
    "# เนื่องจากภาพถูก Normalize ให้มีค่าอยู่ระหว่าง 0 ถึง 1 เราจึงต้องปรับค่ากลับมาอยู่ในช่วง 0 ถึง 1\n",
    "image_augmented = (image_augmented - image_augmented.min()) / (image_augmented.max() - image_augmented.min())\n",
    "\n",
    "# แสดงผลภาพ\n",
    "plt.imshow(image_augmented)\n",
    "plt.axis('off')  # ปิดการแสดงแกน x และ y\n",
    "plt.title(\"Augmented Image\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "m5pPdAcQmSVk",
    "outputId": "84aa38a5-a0b7-4569-8ad8-f1889b73db90"
   },
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# กำหนด transforms สำหรับการทำ Data Augmentation\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),  # ครอบตัดภาพให้มีขนาด 224x224 แบบสุ่ม\n",
    "    transforms.RandomHorizontalFlip(),  # กลับด้านภาพแนวนอนแบบสุ่ม\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # ปรับค่าความสว่าง สีสัน ฯลฯ\n",
    "    transforms.ToTensor(),  # แปลงภาพให้เป็น Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ปรับค่า Normalize\n",
    "])\n",
    "\n",
    "# ฟังก์ชันในการโหลดและแสดงผลภาพ\n",
    "def show_augmented_image(image_path, title):\n",
    "    image = Image.open(image_path).convert('RGB')  # Ensure image is in RGB format\n",
    "    image_augmented = data_augmentation(image)\n",
    "    image_augmented = image_augmented.permute(1, 2, 0).numpy()\n",
    "    image_augmented = (image_augmented - image_augmented.min()) / (image_augmented.max() - image_augmented.min())\n",
    "\n",
    "    plt.imshow(image_augmented)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Directory path และ corresponding class index\n",
    "image_directories = {\n",
    "    \"AD\": '/content/test/AD',\n",
    "    \"PD\": '/content/test/PD',\n",
    "    \"CONTROL\": '/content/test/CONTROL'\n",
    "}\n",
    "\n",
    "# แสดงผลภาพจากแต่ละหมวดหมู่\n",
    "for category, directory in image_directories.items():\n",
    "    image_paths = [os.path.join(directory, filename) for filename in os.listdir(directory) if filename.endswith('.png')]\n",
    "    if image_paths:\n",
    "        show_augmented_image(image_paths[0], category)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 726
    },
    "id": "VcN0ZmoBnZUW",
    "outputId": "4bc8681a-f776-4344-eb9a-5754db668562"
   },
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define transforms for specific augmentations\n",
    "rotation_45 = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=45),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "rotation_135 = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=135),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "zoom_in = transforms.Compose([\n",
    "    transforms.Resize((int(224 * 1.1), int(224 * 1.1))),  # 10% Zoom\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Function to load and show the augmented images\n",
    "def show_augmented_images(image_path, title):\n",
    "    image = Image.open(image_path).convert('RGB')  # Ensure image is in RGB format\n",
    "\n",
    "    # Apply augmentations\n",
    "    image_45 = rotation_45(image)\n",
    "    image_135 = rotation_135(image)\n",
    "    image_zoom = zoom_in(image)\n",
    "\n",
    "    # Convert tensors to numpy arrays for displaying\n",
    "    image_45 = image_45.permute(1, 2, 0).numpy()\n",
    "    image_135 = image_135.permute(1, 2, 0).numpy()\n",
    "    image_zoom = image_zoom.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Normalize the images for displaying\n",
    "    images = [image_45, image_135, image_zoom]\n",
    "    images = [(img - img.min()) / (img.max() - img.min()) for img in images]\n",
    "\n",
    "    # Plotting the images\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    for ax, img, label in zip(axes, images, ['Rotation 45°', 'Rotation 135°', 'Zoom 10%']):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'{title} - {label}')\n",
    "    plt.show()\n",
    "\n",
    "# Directory path and corresponding class index\n",
    "image_directories = {\n",
    "    \"AD\": '/content/test/AD',\n",
    "    \"PD\": '/content/test/PD',\n",
    "    \"CONTROL\": '/content/test/CONTROL'\n",
    "}\n",
    "\n",
    "# Display images from each category\n",
    "for category, directory in image_directories.items():\n",
    "    image_paths = [os.path.join(directory, filename) for filename in os.listdir(directory) if filename.endswith('.png')]\n",
    "    if image_paths:\n",
    "        show_augmented_images(image_paths[0], category)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nkWkkDRx4zbx",
    "outputId": "cecf4a8f-18a7-4c47-8873-bdd6592471ce"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# สมมติให้ y_true คือข้อมูล label จริง และ y_pred คือผลลัพธ์ที่โมเดลทำนายได้\n",
    "# y_true และ y_pred จะเป็น array ที่มีค่าเป็น label (AD, CONTROL, PD) ของข้อมูลแต่ละตัวอย่าง\n",
    "\n",
    "# ตัวอย่างของ y_true และ y_pred (คุณต้องแทนที่ด้วยค่าจริงที่ได้จากโมเดลของคุณ)\n",
    "# Replace ellipsis with actual label values\n",
    "y_true = np.array(['AD', 'CONTROL', 'PD', 'CONTROL', 'AD', 'PD', 'AD'])\n",
    "y_pred = np.array(['AD', 'AD', 'PD', 'CONTROL', 'CONTROL', 'PD', 'CONTROL'])\n",
    "\n",
    "# คำนวณค่า confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=['AD', 'CONTROL', 'PD'])\n",
    "\n",
    "# คำนวณค่า Precision, Recall, F1 Score, และ Accuracy\n",
    "report = classification_report(y_true, y_pred, target_names=['AD', 'CONTROL', 'PD'])\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# แสดงผลลัพธ์\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHzGDS0L5BJV",
    "outputId": "7262eac1-0708-46bf-96ee-432b08702e9c"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "import timm\n",
    "\n",
    "# กำหนด transforms สำหรับการทำ Data Augmentation\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# โหลด dataset\n",
    "dataset = datasets.ImageFolder(root='/content/test', transform=data_transforms)\n",
    "\n",
    "# แบ่ง dataset เป็น train และ test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# สร้าง DataLoader สำหรับ train และ test\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# เลือกโมเดลที่จะใช้ (ในที่นี้ใช้ tf_efficientnetv2_b0.in1k จาก timm)\n",
    "model = timm.create_model('tf_efficientnetv2_b0.in1k', pretrained=True)\n",
    "model.classifier = nn.Linear(model.classifier.in_features, 3)  # กำหนด output ให้เท่ากับจำนวน class ที่มี\n",
    "\n",
    "# กำหนด loss function และ optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# การฝึกโมเดล\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# การทดสอบโมเดล\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(preds.numpy())\n",
    "\n",
    "# คำนวณค่า Precision, Recall, F1 Score, และ Accuracy\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=dataset.classes))\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "000ab3594fb24b6a9fa874a5fe27bd53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a145e65d1544b6fa721fea4dc998bbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ef803dbb220477092e0feb59e7e1a7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50c1108f20a841cc89c50a9bc93659c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ef803dbb220477092e0feb59e7e1a7b",
      "placeholder": "​",
      "style": "IPY_MODEL_3a145e65d1544b6fa721fea4dc998bbf",
      "value": " 28.8M/28.8M [00:00&lt;00:00, 57.0MB/s]"
     }
    },
    "6f3f0e38b66c46cd8372daf77db532d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_91b9d63c698b46a0872a424d4e4466fe",
       "IPY_MODEL_fcd4cfa89d6a4bd4b98af6f66dff973f",
       "IPY_MODEL_50c1108f20a841cc89c50a9bc93659c2"
      ],
      "layout": "IPY_MODEL_aa9f9897019c4b9096324ffbc7fc531d"
     }
    },
    "91b9d63c698b46a0872a424d4e4466fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_000ab3594fb24b6a9fa874a5fe27bd53",
      "placeholder": "​",
      "style": "IPY_MODEL_f1e62898af8347dea698e427a24e231c",
      "value": "model.safetensors: 100%"
     }
    },
    "aa9f9897019c4b9096324ffbc7fc531d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c663a41174b84d00a2d998b490cf7e7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6a7a98667d24dc0a529b03d14eee8af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f1e62898af8347dea698e427a24e231c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fcd4cfa89d6a4bd4b98af6f66dff973f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c663a41174b84d00a2d998b490cf7e7b",
      "max": 28840352,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d6a7a98667d24dc0a529b03d14eee8af",
      "value": 28840352
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
